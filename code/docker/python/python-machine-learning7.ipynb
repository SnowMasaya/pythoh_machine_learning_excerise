{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:56\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "basepath = \"./data/aclImdb\"\n",
    "\n",
    "labels = {\"pos\": 1, \"neg\": 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in (\"test\", \"train\"):\n",
    "    for l in (\"pos\", \"neg\"):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = [\"review\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who ARE the people that star in this thing? Ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Really, average is the only word that comes to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First off... I never considered myself an Uwe ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Who ARE the people that star in this thing? Ne...          1\n",
       "1  Really, average is the only word that comes to...          0\n",
       "2  First off... I never considered myself an Uwe ...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df.to_csv(\"./movie_data.csv\", index=False)\n",
    "\n",
    "df = pd.read_csv(\"./movie_data.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\"The sun is shining\", \n",
    "                 \"The weather is sweet\", \n",
    "                 \"The sun is shining the weather is sweet, and one and one is two\"])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sweet': 5, 'weather': 8, 'sun': 4, 'the': 6, 'is': 1, 'shining': 3, 'two': 7, 'one': 2, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.    0.56  0.56  0.    0.43  0.    0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.    0.56]\n",
      " [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm=\"l2\", smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenizer(\"runner like running and thus they run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter(\"runner like running and thus they run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "[w for w in tokenizer_porter(\"a runner likes running and runs a lot\")[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression model for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, \"review\"].values\n",
    "y_train = df.loc[:25000, \"sentiment\"].values\n",
    "X_test = df.loc[25000:, \"review\"].values\n",
    "y_test = df.loc[25000:, \"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   18.3s\n"
     ]
    },
    {
     "ename": "JoblibIndexError",
     "evalue": "JoblibIndexError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='IPython.kernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f341c93a4b0, file \"/...ist-packages/IPython/kernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'IPython.kernel', '__spec__': ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), 'app': <module 'IPython.kernel.zmq.kernelapp' from '/us...4/dist-packages/IPython/kernel/zmq/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), pkg_name='IPython.kernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f341c93a4b0, file \"/...ist-packages/IPython/kernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'IPython.kernel', '__spec__': ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), 'app': <module 'IPython.kernel.zmq.kernelapp' from '/us...4/dist-packages/IPython/kernel/zmq/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from IPython.kernel.zmq import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/config/application.py in launch_instance(cls=<class 'IPython.kernel.zmq.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    569         \n    570         If a global instance already exists, this reinitializes and starts it\n    571         \"\"\"\n    572         app = cls.instance(**kwargs)\n    573         app.initialize(argv)\n--> 574         app.start()\n        app.start = <bound method IPKernelApp.start of <IPython.kernel.zmq.kernelapp.IPKernelApp object>>\n    575 \n    576 #-----------------------------------------------------------------------------\n    577 # utility functions, for convenience\n    578 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelapp.py in start(self=<IPython.kernel.zmq.kernelapp.IPKernelApp object>)\n    368     def start(self):\n    369         if self.poller is not None:\n    370             self.poller.start()\n    371         self.kernel.start()\n    372         try:\n--> 373             ioloop.IOLoop.instance().start()\n    374         except KeyboardInterrupt:\n    375             pass\n    376 \n    377 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    247         if self.control_stream:\n    248             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    249 \n    250         def make_dispatcher(stream):\n    251             def dispatcher(msg):\n--> 252                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    253             return dispatcher\n    254 \n    255         for s in self.shell_streams:\n    256             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in dispatch_shell(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}})\n    208         else:\n    209             # ensure default_int_handler during handler call\n    210             sig = signal(SIGINT, default_int_handler)\n    211             self.log.debug(\"%s: %s\", msg_type, msg)\n    212             try:\n--> 213                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <IPython.kernel.zmq.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'097527A6D76A4C3085695492836F77F4']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}}\n    214             except Exception:\n    215                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    216             finally:\n    217                 signal(SIGINT, sig)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in execute_request(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'097527A6D76A4C3085695492836F77F4'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}})\n    357         if not silent:\n    358             self.execution_count += 1\n    359             self._publish_execute_input(code, parent, self.execution_count)\n    360         \n    361         reply_content = self.do_execute(code, silent, store_history,\n--> 362                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    363 \n    364         # Flush output before sending the reply.\n    365         sys.stdout.flush()\n    366         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/ipkernel.py in do_execute(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, code='gs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    176 \n    177         reply_content = {}\n    178         # FIXME: the shell calls the exception handler itself.\n    179         shell._reply_content = None\n    180         try:\n--> 181             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = 'gs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    182         except:\n    183             status = u'error'\n    184             # FIXME: this code right now isn't being used yet by default,\n    185             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, raw_cell='gs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2866                 self.displayhook.exec_result = result\n   2867 \n   2868                 # Execute the user code\n   2869                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2870                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2871                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2872 \n   2873                 # Reset this so later displayed values do not modify the\n   2874                 # ExecutionResult\n   2875                 self.displayhook.exec_result = None\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-12-7c8b397eb30b>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2976                     return True\n   2977 \n   2978             for i, node in enumerate(to_run_interactive):\n   2979                 mod = ast.Interactive([node])\n   2980                 code = compiler(mod, cell_name, \"single\")\n-> 2981                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2982                     return True\n   2983 \n   2984             # Flush softspace\n   2985             if softspace(sys.stdout, 0):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3030         outflag = 1  # happens in more places, so it's easier as default\n   3031         try:\n   3032             try:\n   3033                 self.hooks.pre_run_code_hook()\n   3034                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3035                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pyprind\\nimport pandas as pd\\nimport os \\n\\nb...bar.update()\\ndf.columns = [\"review\", \"sentiment\"]', 'import numpy as np\\n\\nnp.random.seed(0)\\ndf = df.re...\\n\\ndf = pd.read_csv(\"./movie_data.csv\")\\ndf.head(3)', 'import numpy as np\\nfrom sklearn.feature_extracti...and one is two\"])\\nbag = count.fit_transform(docs)', 'print(count.vocabulary_)', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'def tokenizer(text):\\n    return text.split()\\n\\ntokenizer(\"runner like running and thus they run\")', 'from nltk.stem.porter import PorterStemmer\\n\\nport...r_porter(\"runner like running and thus they run\")', 'import nltk\\n\\nnltk.download(\"stopwords\")', 'from nltk.corpus import stopwords\\n\\nstop = stopwo... running and runs a lot\")[-10:] if w not in stop]', 'X_train = df.loc[:25000, \"review\"].values\\ny_trai...alues\\ny_test = df.loc[25000:, \"sentiment\"].values', 'from sklearn.pipeline import Pipeline\\nfrom sklea... verbose=1,\\n                           n_jobs=-1)', 'gs_lr_tfidf.fit(X_train, y_train)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                               re.... I never considered myself an Uwe ...          0, 6: ['runner', 'like', 'running', 'and', 'thus', 'they', 'run'], 7: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 8: True, 9: ['runner', 'like', 'run', 'run', 'lot']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([ \"I really liked the idea of traveling be... now. This movie WILL be on DVD.\"], dtype=object), ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pyprind\\nimport pandas as pd\\nimport os \\n\\nb...bar.update()\\ndf.columns = [\"review\", \"sentiment\"]', 'import numpy as np\\n\\nnp.random.seed(0)\\ndf = df.re...\\n\\ndf = pd.read_csv(\"./movie_data.csv\")\\ndf.head(3)', 'import numpy as np\\nfrom sklearn.feature_extracti...and one is two\"])\\nbag = count.fit_transform(docs)', 'print(count.vocabulary_)', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'def tokenizer(text):\\n    return text.split()\\n\\ntokenizer(\"runner like running and thus they run\")', 'from nltk.stem.porter import PorterStemmer\\n\\nport...r_porter(\"runner like running and thus they run\")', 'import nltk\\n\\nnltk.download(\"stopwords\")', 'from nltk.corpus import stopwords\\n\\nstop = stopwo... running and runs a lot\")[-10:] if w not in stop]', 'X_train = df.loc[:25000, \"review\"].values\\ny_trai...alues\\ny_test = df.loc[25000:, \"sentiment\"].values', 'from sklearn.pipeline import Pipeline\\nfrom sklea... verbose=1,\\n                           n_jobs=-1)', 'gs_lr_tfidf.fit(X_train, y_train)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                               re.... I never considered myself an Uwe ...          0, 6: ['runner', 'like', 'running', 'and', 'thus', 'they', 'run'], 7: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 8: True, 9: ['runner', 'like', 'run', 'run', 'lot']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([ \"I really liked the idea of traveling be... now. This movie WILL be on DVD.\"], dtype=object), ...}\n   3036             finally:\n   3037                 # Reset our crash handler in place\n   3038                 sys.excepthook = old_excepthook\n   3039         except SystemExit as e:\n\n...........................................................................\n/home/<ipython-input-12-7c8b397eb30b> in <module>()\n----> 1 \n      2 \n      3 \n      4 \n      5 \n      6 gs_lr_tfidf.fit(X_train, y_train)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5,\n       estimator=Pipeline(ste..._func=None,\n       scoring='accuracy', verbose=1), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]))\n    591         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    592             Target relative to X for classification or regression;\n    593             None for unsupervised learning.\n    594 \n    595         \"\"\"\n--> 596         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...func=None,\n       scoring='accuracy', verbose=1)>\n        X = array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object)\n        y = array([1, 0, 0, ..., 1, 1, 0])\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>], 'vect__use_idf': [False]}]\n    597 \n    598 \n    599 class RandomizedSearchCV(BaseSearchCV):\n    600     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5,\n       estimator=Pipeline(ste..._func=None,\n       scoring='accuracy', verbose=1), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    373             pre_dispatch=pre_dispatch\n    374         )(\n    375             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    376                                     train, test, self.verbose, parameters,\n    377                                     self.fit_params, return_parameters=True)\n--> 378             for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    379             for train, test in cv)\n    380 \n    381         # Out is a list of triplet: score, estimator, n_test_samples\n    382         n_fits = len(out)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<itertools.islice object>)\n    655             if pre_dispatch == \"all\" or n_jobs == 1:\n    656                 # The iterable was consumed all at once by the above for loop.\n    657                 # No need to wait for async callbacks to trigger to\n    658                 # consumption.\n    659                 self._iterating = False\n--> 660             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    661             # Make sure that we get a last message telling us we are done\n    662             elapsed_time = time.time() - self._start_time\n    663             self._print('Done %3i out of %3i | elapsed: %s finished',\n    664                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    IndexError                                         Mon Mar 27 22:22:23 2017\nPID: 51                                      Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), scorer=make_scorer(accuracy_score), train=array([ 4942,  4948,  4949, ..., 24998, 24999, 25000]), test=array([   0,    1,    2, ..., 5066, 5067, 5068]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer_porter>}, fit_params={}, return_train_score=False, return_parameters=True)\n   1234     X_train, y_train = _safe_split(estimator, X, y, train)\n   1235     X_test, y_test = _safe_split(estimator, X, y, test, train)\n   1236     if y_train is None:\n   1237         estimator.fit(X_train, **fit_params)\n   1238     else:\n-> 1239         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X_train = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y_train = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params = {}\n   1240     test_score = _score(estimator, X_test, y_test, scorer)\n   1241     if return_train_score:\n   1242         train_score = _score(estimator, X_train, y_train, scorer)\n   1243 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    124 \n    125     def fit(self, X, y=None, **fit_params):\n    126         \"\"\"Fit all the transforms one after the other and transform the\n    127         data, then fit the transformed data using the final estimator.\n    128         \"\"\"\n--> 129         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._pre_transform = <bound method Pipeline._pre_transform of Pipelin...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y = array([0, 0, 0, ..., 1, 1, 0])\n    130         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    131         return self\n    132 \n    133     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _pre_transform(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    114             step, param = pname.split('__', 1)\n    115             fit_params_steps[step][param] = pval\n    116         Xt = X\n    117         for name, transform in self.steps[:-1]:\n    118             if hasattr(transform, \"fit_transform\"):\n--> 119                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        y = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params_steps = {'clf': {}, 'vect': {}}\n        name = 'vect'\n    120             else:\n    121                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    122                               .transform(Xt)\n    123         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]))\n   1277         Returns\n   1278         -------\n   1279         X : sparse matrix, [n_samples, n_features]\n   1280             Tf-idf-weighted document-term matrix.\n   1281         \"\"\"\n-> 1282         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        raw_documents = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n   1283         self._tfidf.fit(X)\n   1284         # X is already a transformed view of raw_documents so\n   1285         # we set copy to False\n   1286         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=None)\n    812         max_df = self.max_df\n    813         min_df = self.min_df\n    814         max_features = self.max_features\n    815 \n    816         vocabulary, X = self._count_vocab(raw_documents,\n--> 817                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    818 \n    819         if self.binary:\n    820             X.data.fill(1)\n    821 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), fixed_vocab=False)\n    743         analyze = self.build_analyzer()\n    744         j_indices = _make_int_array()\n    745         indptr = _make_int_array()\n    746         indptr.append(0)\n    747         for doc in raw_documents:\n--> 748             for feature in analyze(doc):\n        feature = 'watch.'\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    749                 try:\n    750                     j_indices.append(vocabulary[feature])\n    751                 except KeyError:\n    752                     # Ignore out-of-vocabulary items for fixed_vocab=True\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in <lambda>(doc=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n    229         elif self.analyzer == 'word':\n    230             stop_words = self.get_stop_words()\n    231             tokenize = self.build_tokenizer()\n    232 \n    233             return lambda doc: self._word_ngrams(\n--> 234                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    235 \n    236         else:\n    237             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    238                              self.analyzer)\n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in tokenizer_porter(text=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in <listcomp>(.0=<list_iterator object>)\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in stem(self=<PorterStemmer>, word='OED')\n    660             # the stemming process, although no mention is made of this\n    661             # in the published algorithm.\n    662             return word\n    663 \n    664         stem = self._step1a(stem)\n--> 665         stem = self._step1b(stem)\n        stem = 'oed'\n        self._step1b = <bound method PorterStemmer._step1b of <PorterStemmer>>\n    666         stem = self._step1c(stem)\n    667         stem = self._step2(stem)\n    668         stem = self._step3(stem)\n    669         stem = self._step4(stem)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _step1b(self=<PorterStemmer>, word='oed')\n    371             ),\n    372             # (m=1 and *o) -> E\n    373             (\n    374                 '',\n    375                 'e',\n--> 376                 lambda stem: (self._measure(stem) == 1 and\n        stem = undefined\n        self._measure = <bound method PorterStemmer._measure of <PorterStemmer>>\n        self._ends_cvc = <bound method PorterStemmer._ends_cvc of <PorterStemmer>>\n    377                               self._ends_cvc(stem))\n    378             ),\n    379         ])\n    380     \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _apply_rule_list(self=<PorterStemmer>, word='o', rules=[('at', 'ate', None), ('bl', 'ble', None), ('iz', 'ize', None), ('*d', 'o', <function PorterStemmer._step1b.<locals>.<lambda>>), ('', 'e', <function PorterStemmer._step1b.<locals>.<lambda>>)])\n    253         final element being the condition for the rule to be applicable,\n    254         or None if the rule is unconditional.\n    255         \"\"\"\n    256         for rule in rules:\n    257             suffix, replacement, condition = rule\n--> 258             if suffix == '*d' and self._ends_double_consonant(word):\n        suffix = '*d'\n        self._ends_double_consonant = <bound method PorterStemmer._ends_double_consonant of <PorterStemmer>>\n        word = 'o'\n    259                 stem = word[:-2]\n    260                 if condition is None or condition(stem):\n    261                     return stem + replacement\n    262                 else:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _ends_double_consonant(self=<PorterStemmer>, word='o')\n    209         \"\"\"Implements condition *d from the paper\n    210         \n    211         Returns True if word ends with a double consonant\n    212         \"\"\"\n    213         return (\n--> 214             word[-1] == word[-2] and\n        word = 'o'\n    215             self._is_consonant(word, len(word)-1)\n    216         )\n    217 \n    218     def _ends_cvc(self, word):\n\nIndexError: string index out of range\n___________________________________________________________________________",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 92, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py\", line 1239, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 129, in fit\n    Xt, fit_params = self._pre_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py\", line 119, in _pre_transform\n    Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\", line 1282, in fit_transform\n    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\", line 817, in fit_transform\n    self.fixed_vocabulary_)\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\", line 748, in _count_vocab\n    for feature in analyze(doc):\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py\", line 234, in <lambda>\n    tokenize(preprocess(self.decode(doc))), stop_words)\n  File \"<ipython-input-7-0a0b85ac469f>\", line 6, in tokenizer_porter\n    return [porter.stem(word) for word in text.split()]\n  File \"<ipython-input-7-0a0b85ac469f>\", line 6, in <listcomp>\n    return [porter.stem(word) for word in text.split()]\n  File \"/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py\", line 665, in stem\n    stem = self._step1b(stem)\n  File \"/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py\", line 376, in _step1b\n    lambda stem: (self._measure(stem) == 1 and\n  File \"/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py\", line 258, in _apply_rule_list\n    if suffix == '*d' and self._ends_double_consonant(word):\n  File \"/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py\", line 214, in _ends_double_consonant\n    word[-1] == word[-2] and\nIndexError: string index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.4/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\", line 102, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nIndexError                                         Mon Mar 27 22:22:23 2017\nPID: 51                                      Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), scorer=make_scorer(accuracy_score), train=array([ 4942,  4948,  4949, ..., 24998, 24999, 25000]), test=array([   0,    1,    2, ..., 5066, 5067, 5068]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer_porter>}, fit_params={}, return_train_score=False, return_parameters=True)\n   1234     X_train, y_train = _safe_split(estimator, X, y, train)\n   1235     X_test, y_test = _safe_split(estimator, X, y, test, train)\n   1236     if y_train is None:\n   1237         estimator.fit(X_train, **fit_params)\n   1238     else:\n-> 1239         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X_train = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y_train = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params = {}\n   1240     test_score = _score(estimator, X_test, y_test, scorer)\n   1241     if return_train_score:\n   1242         train_score = _score(estimator, X_train, y_train, scorer)\n   1243 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    124 \n    125     def fit(self, X, y=None, **fit_params):\n    126         \"\"\"Fit all the transforms one after the other and transform the\n    127         data, then fit the transformed data using the final estimator.\n    128         \"\"\"\n--> 129         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._pre_transform = <bound method Pipeline._pre_transform of Pipelin...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y = array([0, 0, 0, ..., 1, 1, 0])\n    130         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    131         return self\n    132 \n    133     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _pre_transform(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    114             step, param = pname.split('__', 1)\n    115             fit_params_steps[step][param] = pval\n    116         Xt = X\n    117         for name, transform in self.steps[:-1]:\n    118             if hasattr(transform, \"fit_transform\"):\n--> 119                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        y = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params_steps = {'clf': {}, 'vect': {}}\n        name = 'vect'\n    120             else:\n    121                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    122                               .transform(Xt)\n    123         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]))\n   1277         Returns\n   1278         -------\n   1279         X : sparse matrix, [n_samples, n_features]\n   1280             Tf-idf-weighted document-term matrix.\n   1281         \"\"\"\n-> 1282         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        raw_documents = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n   1283         self._tfidf.fit(X)\n   1284         # X is already a transformed view of raw_documents so\n   1285         # we set copy to False\n   1286         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=None)\n    812         max_df = self.max_df\n    813         min_df = self.min_df\n    814         max_features = self.max_features\n    815 \n    816         vocabulary, X = self._count_vocab(raw_documents,\n--> 817                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    818 \n    819         if self.binary:\n    820             X.data.fill(1)\n    821 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), fixed_vocab=False)\n    743         analyze = self.build_analyzer()\n    744         j_indices = _make_int_array()\n    745         indptr = _make_int_array()\n    746         indptr.append(0)\n    747         for doc in raw_documents:\n--> 748             for feature in analyze(doc):\n        feature = 'watch.'\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    749                 try:\n    750                     j_indices.append(vocabulary[feature])\n    751                 except KeyError:\n    752                     # Ignore out-of-vocabulary items for fixed_vocab=True\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in <lambda>(doc=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n    229         elif self.analyzer == 'word':\n    230             stop_words = self.get_stop_words()\n    231             tokenize = self.build_tokenizer()\n    232 \n    233             return lambda doc: self._word_ngrams(\n--> 234                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    235 \n    236         else:\n    237             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    238                              self.analyzer)\n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in tokenizer_porter(text=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in <listcomp>(.0=<list_iterator object>)\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in stem(self=<PorterStemmer>, word='OED')\n    660             # the stemming process, although no mention is made of this\n    661             # in the published algorithm.\n    662             return word\n    663 \n    664         stem = self._step1a(stem)\n--> 665         stem = self._step1b(stem)\n        stem = 'oed'\n        self._step1b = <bound method PorterStemmer._step1b of <PorterStemmer>>\n    666         stem = self._step1c(stem)\n    667         stem = self._step2(stem)\n    668         stem = self._step3(stem)\n    669         stem = self._step4(stem)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _step1b(self=<PorterStemmer>, word='oed')\n    371             ),\n    372             # (m=1 and *o) -> E\n    373             (\n    374                 '',\n    375                 'e',\n--> 376                 lambda stem: (self._measure(stem) == 1 and\n        stem = undefined\n        self._measure = <bound method PorterStemmer._measure of <PorterStemmer>>\n        self._ends_cvc = <bound method PorterStemmer._ends_cvc of <PorterStemmer>>\n    377                               self._ends_cvc(stem))\n    378             ),\n    379         ])\n    380     \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _apply_rule_list(self=<PorterStemmer>, word='o', rules=[('at', 'ate', None), ('bl', 'ble', None), ('iz', 'ize', None), ('*d', 'o', <function PorterStemmer._step1b.<locals>.<lambda>>), ('', 'e', <function PorterStemmer._step1b.<locals>.<lambda>>)])\n    253         final element being the condition for the rule to be applicable,\n    254         or None if the rule is unconditional.\n    255         \"\"\"\n    256         for rule in rules:\n    257             suffix, replacement, condition = rule\n--> 258             if suffix == '*d' and self._ends_double_consonant(word):\n        suffix = '*d'\n        self._ends_double_consonant = <bound method PorterStemmer._ends_double_consonant of <PorterStemmer>>\n        word = 'o'\n    259                 stem = word[:-2]\n    260                 if condition is None or condition(stem):\n    261                     return stem + replacement\n    262                 else:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _ends_double_consonant(self=<PorterStemmer>, word='o')\n    209         \"\"\"Implements condition *d from the paper\n    210         \n    211         Returns True if word ends with a double consonant\n    212         \"\"\"\n    213         return (\n--> 214             word[-1] == word[-2] and\n        word = 'o'\n    215             self._is_consonant(word, len(word)-1)\n    216         )\n    217 \n    218     def _ends_cvc(self, word):\n\nIndexError: string index out of range\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nIndexError                                         Mon Mar 27 22:22:23 2017\nPID: 51                                      Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), scorer=make_scorer(accuracy_score), train=array([ 4942,  4948,  4949, ..., 24998, 24999, 25000]), test=array([   0,    1,    2, ..., 5066, 5067, 5068]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer_porter>}, fit_params={}, return_train_score=False, return_parameters=True)\n   1234     X_train, y_train = _safe_split(estimator, X, y, train)\n   1235     X_test, y_test = _safe_split(estimator, X, y, test, train)\n   1236     if y_train is None:\n   1237         estimator.fit(X_train, **fit_params)\n   1238     else:\n-> 1239         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X_train = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y_train = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params = {}\n   1240     test_score = _score(estimator, X_test, y_test, scorer)\n   1241     if return_train_score:\n   1242         train_score = _score(estimator, X_train, y_train, scorer)\n   1243 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    124 \n    125     def fit(self, X, y=None, **fit_params):\n    126         \"\"\"Fit all the transforms one after the other and transform the\n    127         data, then fit the transformed data using the final estimator.\n    128         \"\"\"\n--> 129         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._pre_transform = <bound method Pipeline._pre_transform of Pipelin...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y = array([0, 0, 0, ..., 1, 1, 0])\n    130         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    131         return self\n    132 \n    133     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _pre_transform(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    114             step, param = pname.split('__', 1)\n    115             fit_params_steps[step][param] = pval\n    116         Xt = X\n    117         for name, transform in self.steps[:-1]:\n    118             if hasattr(transform, \"fit_transform\"):\n--> 119                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        y = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params_steps = {'clf': {}, 'vect': {}}\n        name = 'vect'\n    120             else:\n    121                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    122                               .transform(Xt)\n    123         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]))\n   1277         Returns\n   1278         -------\n   1279         X : sparse matrix, [n_samples, n_features]\n   1280             Tf-idf-weighted document-term matrix.\n   1281         \"\"\"\n-> 1282         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        raw_documents = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n   1283         self._tfidf.fit(X)\n   1284         # X is already a transformed view of raw_documents so\n   1285         # we set copy to False\n   1286         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=None)\n    812         max_df = self.max_df\n    813         min_df = self.min_df\n    814         max_features = self.max_features\n    815 \n    816         vocabulary, X = self._count_vocab(raw_documents,\n--> 817                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    818 \n    819         if self.binary:\n    820             X.data.fill(1)\n    821 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), fixed_vocab=False)\n    743         analyze = self.build_analyzer()\n    744         j_indices = _make_int_array()\n    745         indptr = _make_int_array()\n    746         indptr.append(0)\n    747         for doc in raw_documents:\n--> 748             for feature in analyze(doc):\n        feature = 'watch.'\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    749                 try:\n    750                     j_indices.append(vocabulary[feature])\n    751                 except KeyError:\n    752                     # Ignore out-of-vocabulary items for fixed_vocab=True\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in <lambda>(doc=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n    229         elif self.analyzer == 'word':\n    230             stop_words = self.get_stop_words()\n    231             tokenize = self.build_tokenizer()\n    232 \n    233             return lambda doc: self._word_ngrams(\n--> 234                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    235 \n    236         else:\n    237             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    238                              self.analyzer)\n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in tokenizer_porter(text=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in <listcomp>(.0=<list_iterator object>)\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in stem(self=<PorterStemmer>, word='OED')\n    660             # the stemming process, although no mention is made of this\n    661             # in the published algorithm.\n    662             return word\n    663 \n    664         stem = self._step1a(stem)\n--> 665         stem = self._step1b(stem)\n        stem = 'oed'\n        self._step1b = <bound method PorterStemmer._step1b of <PorterStemmer>>\n    666         stem = self._step1c(stem)\n    667         stem = self._step2(stem)\n    668         stem = self._step3(stem)\n    669         stem = self._step4(stem)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _step1b(self=<PorterStemmer>, word='oed')\n    371             ),\n    372             # (m=1 and *o) -> E\n    373             (\n    374                 '',\n    375                 'e',\n--> 376                 lambda stem: (self._measure(stem) == 1 and\n        stem = undefined\n        self._measure = <bound method PorterStemmer._measure of <PorterStemmer>>\n        self._ends_cvc = <bound method PorterStemmer._ends_cvc of <PorterStemmer>>\n    377                               self._ends_cvc(stem))\n    378             ),\n    379         ])\n    380     \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _apply_rule_list(self=<PorterStemmer>, word='o', rules=[('at', 'ate', None), ('bl', 'ble', None), ('iz', 'ize', None), ('*d', 'o', <function PorterStemmer._step1b.<locals>.<lambda>>), ('', 'e', <function PorterStemmer._step1b.<locals>.<lambda>>)])\n    253         final element being the condition for the rule to be applicable,\n    254         or None if the rule is unconditional.\n    255         \"\"\"\n    256         for rule in rules:\n    257             suffix, replacement, condition = rule\n--> 258             if suffix == '*d' and self._ends_double_consonant(word):\n        suffix = '*d'\n        self._ends_double_consonant = <bound method PorterStemmer._ends_double_consonant of <PorterStemmer>>\n        word = 'o'\n    259                 stem = word[:-2]\n    260                 if condition is None or condition(stem):\n    261                     return stem + replacement\n    262                 else:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _ends_double_consonant(self=<PorterStemmer>, word='o')\n    209         \"\"\"Implements condition *d from the paper\n    210         \n    211         Returns True if word ends with a double consonant\n    212         \"\"\"\n    213         return (\n--> 214             word[-1] == word[-2] and\n        word = 'o'\n    215             self._is_consonant(word, len(word)-1)\n    216         )\n    217 \n    218     def _ends_cvc(self, word):\n\nIndexError: string index out of range\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibIndexError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7c8b397eb30b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs_lr_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    376\u001b[0m                                     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m                                     self.fit_params, return_parameters=True)\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m             for train, test in cv)\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    658\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    541\u001b[0m                         \u001b[1;31m# Convert this to a JoblibException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m                         \u001b[0mexception_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mk_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibIndexError\u001b[0m: JoblibIndexError\n___________________________________________________________________________\nMultiprocessing exception:\n    ...........................................................................\n/usr/lib/python3.4/runpy.py in _run_module_as_main(mod_name='IPython.kernel.__main__', alter_argv=1)\n    165         sys.exit(msg)\n    166     main_globals = sys.modules[\"__main__\"].__dict__\n    167     if alter_argv:\n    168         sys.argv[0] = mod_spec.origin\n    169     return _run_code(code, main_globals, None,\n--> 170                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py')\n    171 \n    172 def run_module(mod_name, init_globals=None,\n    173                run_name=None, alter_sys=False):\n    174     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/usr/lib/python3.4/runpy.py in _run_code(code=<code object <module> at 0x7f341c93a4b0, file \"/...ist-packages/IPython/kernel/__main__.py\", line 1>, run_globals={'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'IPython.kernel', '__spec__': ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), 'app': <module 'IPython.kernel.zmq.kernelapp' from '/us...4/dist-packages/IPython/kernel/zmq/kernelapp.py'>}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), pkg_name='IPython.kernel', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f341c93a4b0, file \"/...ist-packages/IPython/kernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module 'builtins' (built-in)>, '__cached__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__pycache__/__main__.cpython-34.pyc', '__doc__': None, '__file__': '/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py', '__loader__': <_frozen_importlib.SourceFileLoader object>, '__name__': '__main__', '__package__': 'IPython.kernel', '__spec__': ModuleSpec(name='IPython.kernel.__main__', loade...hon3.4/dist-packages/IPython/kernel/__main__.py'), 'app': <module 'IPython.kernel.zmq.kernelapp' from '/us...4/dist-packages/IPython/kernel/zmq/kernelapp.py'>}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from IPython.kernel.zmq import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/config/application.py in launch_instance(cls=<class 'IPython.kernel.zmq.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    569         \n    570         If a global instance already exists, this reinitializes and starts it\n    571         \"\"\"\n    572         app = cls.instance(**kwargs)\n    573         app.initialize(argv)\n--> 574         app.start()\n        app.start = <bound method IPKernelApp.start of <IPython.kernel.zmq.kernelapp.IPKernelApp object>>\n    575 \n    576 #-----------------------------------------------------------------------------\n    577 # utility functions, for convenience\n    578 #-----------------------------------------------------------------------------\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelapp.py in start(self=<IPython.kernel.zmq.kernelapp.IPKernelApp object>)\n    368     def start(self):\n    369         if self.poller is not None:\n    370             self.poller.start()\n    371         self.kernel.start()\n    372         try:\n--> 373             ioloop.IOLoop.instance().start()\n    374         except KeyboardInterrupt:\n    375             pass\n    376 \n    377 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    157             PollIOLoop.configure(ZMQIOLoop)\n    158         return PollIOLoop.current(*args, **kwargs)\n    159     \n    160     def start(self):\n    161         try:\n--> 162             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    163         except ZMQError as e:\n    164             if e.errno == ETERM:\n    165                 # quietly return on ETERM\n    166                 pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    247         if self.control_stream:\n    248             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    249 \n    250         def make_dispatcher(stream):\n    251             def dispatcher(msg):\n--> 252                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    253             return dispatcher\n    254 \n    255         for s in self.shell_streams:\n    256             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in dispatch_shell(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}})\n    208         else:\n    209             # ensure default_int_handler during handler call\n    210             sig = signal(SIGINT, default_int_handler)\n    211             self.log.debug(\"%s: %s\", msg_type, msg)\n    212             try:\n--> 213                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <IPython.kernel.zmq.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'097527A6D76A4C3085695492836F77F4']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}}\n    214             except Exception:\n    215                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    216             finally:\n    217                 signal(SIGINT, sig)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/kernelbase.py in execute_request(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'097527A6D76A4C3085695492836F77F4'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'gs_lr_tfidf.fit(X_train, y_train)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'session': '097527A6D76A4C3085695492836F77F4', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '5EB6E9184D4847FC81967D8606E502E8', 'msg_type': 'execute_request', 'parent_header': {}})\n    357         if not silent:\n    358             self.execution_count += 1\n    359             self._publish_execute_input(code, parent, self.execution_count)\n    360         \n    361         reply_content = self.do_execute(code, silent, store_history,\n--> 362                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    363 \n    364         # Flush output before sending the reply.\n    365         sys.stdout.flush()\n    366         sys.stderr.flush()\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/kernel/zmq/ipkernel.py in do_execute(self=<IPython.kernel.zmq.ipkernel.IPythonKernel object>, code='gs_lr_tfidf.fit(X_train, y_train)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    176 \n    177         reply_content = {}\n    178         # FIXME: the shell calls the exception handler itself.\n    179         shell._reply_content = None\n    180         try:\n--> 181             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = 'gs_lr_tfidf.fit(X_train, y_train)'\n        store_history = True\n        silent = False\n    182         except:\n    183             status = u'error'\n    184             # FIXME: this code right now isn't being used yet by default,\n    185             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_cell(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, raw_cell='gs_lr_tfidf.fit(X_train, y_train)', store_history=True, silent=False, shell_futures=True)\n   2866                 self.displayhook.exec_result = result\n   2867 \n   2868                 # Execute the user code\n   2869                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2870                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2871                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2872 \n   2873                 # Reset this so later displayed values do not modify the\n   2874                 # ExecutionResult\n   2875                 self.displayhook.exec_result = None\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>], cell_name='<ipython-input-12-7c8b397eb30b>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   2976                     return True\n   2977 \n   2978             for i, node in enumerate(to_run_interactive):\n   2979                 mod = ast.Interactive([node])\n   2980                 code = compiler(mod, cell_name, \"single\")\n-> 2981                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <I....kernel.zmq.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   2982                     return True\n   2983 \n   2984             # Flush softspace\n   2985             if softspace(sys.stdout, 0):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/IPython/core/interactiveshell.py in run_code(self=<IPython.kernel.zmq.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3030         outflag = 1  # happens in more places, so it's easier as default\n   3031         try:\n   3032             try:\n   3033                 self.hooks.pre_run_code_hook()\n   3034                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3035                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f33c8ac0d20, file \"<ipython-input-12-7c8b397eb30b>\", line 1>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pyprind\\nimport pandas as pd\\nimport os \\n\\nb...bar.update()\\ndf.columns = [\"review\", \"sentiment\"]', 'import numpy as np\\n\\nnp.random.seed(0)\\ndf = df.re...\\n\\ndf = pd.read_csv(\"./movie_data.csv\")\\ndf.head(3)', 'import numpy as np\\nfrom sklearn.feature_extracti...and one is two\"])\\nbag = count.fit_transform(docs)', 'print(count.vocabulary_)', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'def tokenizer(text):\\n    return text.split()\\n\\ntokenizer(\"runner like running and thus they run\")', 'from nltk.stem.porter import PorterStemmer\\n\\nport...r_porter(\"runner like running and thus they run\")', 'import nltk\\n\\nnltk.download(\"stopwords\")', 'from nltk.corpus import stopwords\\n\\nstop = stopwo... running and runs a lot\")[-10:] if w not in stop]', 'X_train = df.loc[:25000, \"review\"].values\\ny_trai...alues\\ny_test = df.loc[25000:, \"sentiment\"].values', 'from sklearn.pipeline import Pipeline\\nfrom sklea... verbose=1,\\n                           n_jobs=-1)', 'gs_lr_tfidf.fit(X_train, y_train)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                               re.... I never considered myself an Uwe ...          0, 6: ['runner', 'like', 'running', 'and', 'thus', 'they', 'run'], 7: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 8: True, 9: ['runner', 'like', 'run', 'run', 'lot']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([ \"I really liked the idea of traveling be... now. This movie WILL be on DVD.\"], dtype=object), ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'GridSearchCV': <class 'sklearn.grid_search.GridSearchCV'>, 'In': ['', 'import pyprind\\nimport pandas as pd\\nimport os \\n\\nb...bar.update()\\ndf.columns = [\"review\", \"sentiment\"]', 'import numpy as np\\n\\nnp.random.seed(0)\\ndf = df.re...\\n\\ndf = pd.read_csv(\"./movie_data.csv\")\\ndf.head(3)', 'import numpy as np\\nfrom sklearn.feature_extracti...and one is two\"])\\nbag = count.fit_transform(docs)', 'print(count.vocabulary_)', 'from sklearn.feature_extraction.text import Tfid...t_transform(count.fit_transform(docs)).toarray())', 'def tokenizer(text):\\n    return text.split()\\n\\ntokenizer(\"runner like running and thus they run\")', 'from nltk.stem.porter import PorterStemmer\\n\\nport...r_porter(\"runner like running and thus they run\")', 'import nltk\\n\\nnltk.download(\"stopwords\")', 'from nltk.corpus import stopwords\\n\\nstop = stopwo... running and runs a lot\")[-10:] if w not in stop]', 'X_train = df.loc[:25000, \"review\"].values\\ny_trai...alues\\ny_test = df.loc[25000:, \"sentiment\"].values', 'from sklearn.pipeline import Pipeline\\nfrom sklea... verbose=1,\\n                           n_jobs=-1)', 'gs_lr_tfidf.fit(X_train, y_train)'], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {2:                                               re.... I never considered myself an Uwe ...          0, 6: ['runner', 'like', 'running', 'and', 'thus', 'they', 'run'], 7: ['runner', 'like', 'run', 'and', 'thu', 'they', 'run'], 8: True, 9: ['runner', 'like', 'run', 'run', 'lot']}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'TfidfTransformer': <class 'sklearn.feature_extraction.text.TfidfTransformer'>, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, 'X_test': array([ \"I really liked the idea of traveling be... now. This movie WILL be on DVD.\"], dtype=object), ...}\n   3036             finally:\n   3037                 # Reset our crash handler in place\n   3038                 sys.excepthook = old_excepthook\n   3039         except SystemExit as e:\n\n...........................................................................\n/home/<ipython-input-12-7c8b397eb30b> in <module>()\n----> 1 \n      2 \n      3 \n      4 \n      5 \n      6 gs_lr_tfidf.fit(X_train, y_train)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py in fit(self=GridSearchCV(cv=5,\n       estimator=Pipeline(ste..._func=None,\n       scoring='accuracy', verbose=1), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]))\n    591         y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n    592             Target relative to X for classification or regression;\n    593             None for unsupervised learning.\n    594 \n    595         \"\"\"\n--> 596         return self._fit(X, y, ParameterGrid(self.param_grid))\n        self._fit = <bound method GridSearchCV._fit of GridSearchCV(...func=None,\n       scoring='accuracy', verbose=1)>\n        X = array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object)\n        y = array([1, 0, 0, ..., 1, 1, 0])\n        self.param_grid = [{'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>]}, {'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2'], 'vect__ngram_range': [(1, 1)], 'vect__norm': [None], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], None], 'vect__tokenizer': [<function tokenizer>, <function tokenizer_porter>], 'vect__use_idf': [False]}]\n    597 \n    598 \n    599 class RandomizedSearchCV(BaseSearchCV):\n    600     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py in _fit(self=GridSearchCV(cv=5,\n       estimator=Pipeline(ste..._func=None,\n       scoring='accuracy', verbose=1), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), parameter_iterable=<sklearn.grid_search.ParameterGrid object>)\n    373             pre_dispatch=pre_dispatch\n    374         )(\n    375             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    376                                     train, test, self.verbose, parameters,\n    377                                     self.fit_params, return_parameters=True)\n--> 378             for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterGrid object>\n    379             for train, test in cv)\n    380 \n    381         # Out is a list of triplet: score, estimator, n_test_samples\n    382         n_fits = len(out)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<itertools.islice object>)\n    655             if pre_dispatch == \"all\" or n_jobs == 1:\n    656                 # The iterable was consumed all at once by the above for loop.\n    657                 # No need to wait for async callbacks to trigger to\n    658                 # consumption.\n    659                 self._iterating = False\n--> 660             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    661             # Make sure that we get a last message telling us we are done\n    662             elapsed_time = time.time() - self._start_time\n    663             self._print('Done %3i out of %3i | elapsed: %s finished',\n    664                         (len(self._output),\n\n    ---------------------------------------------------------------------------\n    Sub-process traceback:\n    ---------------------------------------------------------------------------\n    IndexError                                         Mon Mar 27 22:22:23 2017\nPID: 51                                      Python 3.4.3: /usr/bin/python3\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py in _fit_and_score(estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'Who ARE the people that star in this th...an a tired band of knit pickers.\"], dtype=object), y=array([1, 0, 0, ..., 1, 1, 0]), scorer=make_scorer(accuracy_score), train=array([ 4942,  4948,  4949, ..., 24998, 24999, 25000]), test=array([   0,    1,    2, ..., 5066, 5067, 5068]), verbose=1, parameters={'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', ...], 'vect__tokenizer': <function tokenizer_porter>}, fit_params={}, return_train_score=False, return_parameters=True)\n   1234     X_train, y_train = _safe_split(estimator, X, y, train)\n   1235     X_test, y_test = _safe_split(estimator, X, y, test, train)\n   1236     if y_train is None:\n   1237         estimator.fit(X_train, **fit_params)\n   1238     else:\n-> 1239         estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method Pipeline.fit of Pipeline(steps=[('...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X_train = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y_train = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params = {}\n   1240     test_score = _score(estimator, X_test, y_test, scorer)\n   1241     if return_train_score:\n   1242         train_score = _score(estimator, X_train, y_train, scorer)\n   1243 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in fit(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    124 \n    125     def fit(self, X, y=None, **fit_params):\n    126         \"\"\"Fit all the transforms one after the other and transform the\n    127         data, then fit the transformed data using the final estimator.\n    128         \"\"\"\n--> 129         Xt, fit_params = self._pre_transform(X, y, **fit_params)\n        Xt = undefined\n        fit_params = {}\n        self._pre_transform = <bound method Pipeline._pre_transform of Pipelin...=1, penalty='l1', random_state=0, tol=0.0001))])>\n        X = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        y = array([0, 0, 0, ..., 1, 1, 0])\n    130         self.steps[-1][-1].fit(Xt, y, **fit_params)\n    131         return self\n    132 \n    133     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/pipeline.py in _pre_transform(self=Pipeline(steps=[('vect', TfidfVectorizer(analyze...g=1, penalty='l1', random_state=0, tol=0.0001))]), X=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]), **fit_params={})\n    114             step, param = pname.split('__', 1)\n    115             fit_params_steps[step][param] = pval\n    116         Xt = X\n    117         for name, transform in self.steps[:-1]:\n    118             if hasattr(transform, \"fit_transform\"):\n--> 119                 Xt = transform.fit_transform(Xt, y, **fit_params_steps[name])\n        Xt = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n        transform.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        y = array([0, 0, 0, ..., 1, 1, 0])\n        fit_params_steps = {'clf': {}, 'vect': {}}\n        name = 'vect'\n    120             else:\n    121                 Xt = transform.fit(Xt, y, **fit_params_steps[name]) \\\n    122                               .transform(Xt)\n    123         return Xt, fit_params_steps[self.steps[-1][0]]\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=array([0, 0, 0, ..., 1, 1, 0]))\n   1277         Returns\n   1278         -------\n   1279         X : sparse matrix, [n_samples, n_features]\n   1280             Tf-idf-weighted document-term matrix.\n   1281         \"\"\"\n-> 1282         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...f202378>,\n        use_idf=True, vocabulary=None)>\n        raw_documents = array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object)\n   1283         self._tfidf.fit(X)\n   1284         # X is already a transformed view of raw_documents so\n   1285         # we set copy to False\n   1286         return self._tfidf.transform(X, copy=False)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), y=None)\n    812         max_df = self.max_df\n    813         min_df = self.min_df\n    814         max_features = self.max_features\n    815 \n    816         vocabulary, X = self._count_vocab(raw_documents,\n--> 817                                           self.fixed_vocabulary_)\n        self.fixed_vocabulary_ = False\n    818 \n    819         if self.binary:\n    820             X.data.fill(1)\n    821 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in _count_vocab(self=TfidfVectorizer(analyzer='word', binary=False, c...cf202378>,\n        use_idf=True, vocabulary=None), raw_documents=array([ 'I have nothing to comment on this movie...an a tired band of knit pickers.\"], dtype=object), fixed_vocab=False)\n    743         analyze = self.build_analyzer()\n    744         j_indices = _make_int_array()\n    745         indptr = _make_int_array()\n    746         indptr.append(0)\n    747         for doc in raw_documents:\n--> 748             for feature in analyze(doc):\n        feature = 'watch.'\n        analyze = <function VectorizerMixin.build_analyzer.<locals>.<lambda>>\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    749                 try:\n    750                     j_indices.append(vocabulary[feature])\n    751                 except KeyError:\n    752                     # Ignore out-of-vocabulary items for fixed_vocab=True\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/sklearn/feature_extraction/text.py in <lambda>(doc=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n    229         elif self.analyzer == 'word':\n    230             stop_words = self.get_stop_words()\n    231             tokenize = self.build_tokenizer()\n    232 \n    233             return lambda doc: self._word_ngrams(\n--> 234                 tokenize(preprocess(self.decode(doc))), stop_words)\n        doc = \"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\"\n    235 \n    236         else:\n    237             raise ValueError('%s is not a valid tokenization scheme/analyzer' %\n    238                              self.analyzer)\n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in tokenizer_porter(text=\"This mini-series is actually more entertaining t...er's and the OED consider it an alternative form.\")\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/home/<ipython-input-7-0a0b85ac469f> in <listcomp>(.0=<list_iterator object>)\n      1 from nltk.stem.porter import PorterStemmer\n      2 \n      3 porter = PorterStemmer()\n      4 \n      5 def tokenizer_porter(text):\n----> 6     return [porter.stem(word) for word in text.split()]\n      7 \n      8 tokenizer_porter(\"runner like running and thus they run\")\n      9 \n     10 \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in stem(self=<PorterStemmer>, word='OED')\n    660             # the stemming process, although no mention is made of this\n    661             # in the published algorithm.\n    662             return word\n    663 \n    664         stem = self._step1a(stem)\n--> 665         stem = self._step1b(stem)\n        stem = 'oed'\n        self._step1b = <bound method PorterStemmer._step1b of <PorterStemmer>>\n    666         stem = self._step1c(stem)\n    667         stem = self._step2(stem)\n    668         stem = self._step3(stem)\n    669         stem = self._step4(stem)\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _step1b(self=<PorterStemmer>, word='oed')\n    371             ),\n    372             # (m=1 and *o) -> E\n    373             (\n    374                 '',\n    375                 'e',\n--> 376                 lambda stem: (self._measure(stem) == 1 and\n        stem = undefined\n        self._measure = <bound method PorterStemmer._measure of <PorterStemmer>>\n        self._ends_cvc = <bound method PorterStemmer._ends_cvc of <PorterStemmer>>\n    377                               self._ends_cvc(stem))\n    378             ),\n    379         ])\n    380     \n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _apply_rule_list(self=<PorterStemmer>, word='o', rules=[('at', 'ate', None), ('bl', 'ble', None), ('iz', 'ize', None), ('*d', 'o', <function PorterStemmer._step1b.<locals>.<lambda>>), ('', 'e', <function PorterStemmer._step1b.<locals>.<lambda>>)])\n    253         final element being the condition for the rule to be applicable,\n    254         or None if the rule is unconditional.\n    255         \"\"\"\n    256         for rule in rules:\n    257             suffix, replacement, condition = rule\n--> 258             if suffix == '*d' and self._ends_double_consonant(word):\n        suffix = '*d'\n        self._ends_double_consonant = <bound method PorterStemmer._ends_double_consonant of <PorterStemmer>>\n        word = 'o'\n    259                 stem = word[:-2]\n    260                 if condition is None or condition(stem):\n    261                     return stem + replacement\n    262                 else:\n\n...........................................................................\n/usr/local/lib/python3.4/dist-packages/nltk/stem/porter.py in _ends_double_consonant(self=<PorterStemmer>, word='o')\n    209         \"\"\"Implements condition *d from the paper\n    210         \n    211         Returns True if word ends with a double consonant\n    212         \"\"\"\n    213         return (\n--> 214             word[-1] == word[-2] and\n        word = 'o'\n    215             self._is_consonant(word, len(word)-1)\n    216         )\n    217 \n    218     def _ends_cvc(self, word):\n\nIndexError: string index out of range\n___________________________________________________________________________"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6,  0.4,  0.6,  0.2,  0.6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=6)\n",
    "y = [np.random.randint(3) for i in range(25)]\n",
    "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
    "\n",
    "cv5_idx = list(StratifiedKFold(y, n_folds=5, shuffle=False, random_state=0))\n",
    "\n",
    "cross_val_score(LogisticRegression(random_state=123), X, y, cv=cv5_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.600000 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.400000 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.600000 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.200000 -   0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................................... , score=0.600000 -   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), {}, cv=cv5_idx, verbose=3).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n",
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(cross_val_score(LogisticRegression(), X, y, cv=cv5_idx).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text.lower())\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text , label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"Who ARE the people that star in this thing? Never heard of them!! But this is one of the funniest comedies I have run across. It should win the Putz Puller Prize for Parody. The absurd starts with Dr. Jeykl snorting his powder and turning into a sex fiend.He is pursued by libido driven nurse early in the movie in one of the funniest scenes of the movie. Pay attention to the hospital PA system in the background; rather like the system in MASH. The final scene with Hyde accepting the award has had me laughing for years. Oh... and the \"\"Busty Nurse\"\" is Cassandra Peterson, who went on to become Elvira, Mistress of the Dark. <br /><br />If you liked the Mel Brooks classic movies (Blazing Saddles, etc.), I suspect you\\'d like this one.<br /><br />Damn shame you can\\'t get it on DVD anywhere.<br /><br />It\\'s available on DVD now !!!!! Good thing DVDs don\\'t wear out from use !!!!!\"',\n",
       " 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path=\"./movie_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "            return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error=\"ignore\",\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                        tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path=\"./movie_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:16\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(\"Accuracy: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os \n",
    "\n",
    "dest = os.path.join(\"movieclassifier\", \"pkl_objects\")\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop, open(os.path.join(dest, \"stopwords.pkl\"), \"wb\"))\n",
    "pickle.dump(clf, open(os.path.join(dest, \"classifier.pkl\"), \"wb\"), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "cl = pickle.load(open(os.path.join(\"movieclassifier\", \"pkl_objects\", \"classifier.pkl\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = {0: \"negative\", 1:\"positive\"}\n",
    "example = [\"I love this movie\"]\n",
    "X = vect.transform(example)\n",
    "print(\"Prediction: %s\\nProbability: %.2f%%\" %\\(label[clf]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}